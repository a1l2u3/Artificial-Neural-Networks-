{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5xa9EFPczztS1Tv3aDLJy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a1l2u3/Artificial-Neural-Networks-/blob/ANN/ANN_Practical_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Implement Artificial Neural Network training process in Python by using Forward Propagation,\n",
        "Back Propagation."
      ],
      "metadata": {
        "id": "X8lr2R3fB2F8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1]Initialize Neural Network:\n",
        "\n",
        "Define the neural network architecture, including the number of layers, number of neurons in each layer, activation functions, and learning rate.\n",
        "Initialize weights and biases randomly or using specific initialization techniques.\n",
        "2]Forward Propagation:\n",
        "\n",
        "Perform forward propagation to compute the predicted output for each input sample.\n",
        "For each sample:\n",
        "Compute the weighted sum of inputs to each neuron in the hidden layers and the output layer.\n",
        "Apply the activation function to the weighted sum to obtain the output of each neuron.\n",
        "Pass the output of one layer as input to the next layer until the output layer is reached.\n",
        "3]Compute Loss:\n",
        "\n",
        "Calculate the loss between the predicted output and the actual output using a suitable loss function (e.g., Mean Squared Error).\n",
        "4]Backpropagation:\n",
        "\n",
        "Perform backpropagation to compute the gradients of the loss function with respect to the weights and biases.\n",
        "For each sample:\n",
        "Compute the error (gradient of the loss) at the output layer.\n",
        "Backpropagate the error through the network to compute the gradients of the loss with respect to the weights and biases of each layer.\n",
        "Update the weights and biases using the gradients and the learning rate.\n",
        "5]Repeat:\n",
        "\n",
        "Repeat steps 2 to 4 for a fixed number of iterations (epochs) or until convergence criteria are met.\n",
        "6]Training Evaluation:\n",
        "\n",
        "Monitor the training loss and other metrics (e.g., accuracy) on a validation set to assess the performance of the model during training."
      ],
      "metadata": {
        "id": "WUqSD1bNCT-G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "juuGpHV5B1d1",
        "outputId": "468a37e0-a436-4c1b-9dee-261ba23f3aaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.118886424981326\n",
            "Epoch 100, Loss: nan\n",
            "Epoch 200, Loss: nan\n",
            "Epoch 300, Loss: nan\n",
            "Epoch 400, Loss: nan\n",
            "Epoch 500, Loss: nan\n",
            "Epoch 600, Loss: nan\n",
            "Epoch 700, Loss: nan\n",
            "Epoch 800, Loss: nan\n",
            "Epoch 900, Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-e965fa841f2a>:34: RuntimeWarning: overflow encountered in multiply\n",
            "  d_output = error * self.sigmoid_derivative(output)\n",
            "<ipython-input-5-e965fa841f2a>:37: RuntimeWarning: invalid value encountered in multiply\n",
            "  d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_output)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.bias_input_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.bias_hidden_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        # Forward propagation from input to hidden layer\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_input_hidden\n",
        "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
        "\n",
        "        # Forward propagation from hidden to output layer\n",
        "        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output\n",
        "        return self.output\n",
        "\n",
        "    def backward_propagation(self, X, y, output):\n",
        "        # Backpropagation and update weights and biases\n",
        "        error = y - output\n",
        "        d_output = error * self.sigmoid_derivative(output)\n",
        "\n",
        "        error_hidden_layer = d_output.dot(self.weights_hidden_output.T)\n",
        "        d_hidden_layer = error_hidden_layer * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * self.learning_rate\n",
        "        self.bias_hidden_output += np.sum(d_output, axis=0, keepdims=True) * self.learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(d_hidden_layer) * self.learning_rate\n",
        "        self.bias_input_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * self.learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward propagation\n",
        "            output = self.forward_propagation(X)\n",
        "\n",
        "            # Backward propagation and update weights and biases\n",
        "            self.backward_propagation(X, y, output)\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Define neural network parameters\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "# Create and train the neural network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
        "nn.train(X, y, epochs)\n"
      ]
    }
  ]
}